{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1sgBllAOybm"
   },
   "source": [
    "# Import nessesary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pX8dP29DlJ6"
   },
   "outputs": [],
   "source": [
    "!pip install vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jKQfcEpDrRN"
   },
   "outputs": [],
   "source": [
    "!pip install vncorenlp transformers==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1KyKKH8Dyuz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "from csv import reader\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import traceback\n",
    "import time\n",
    "import requests\n",
    "# from pyvi import ViTokenizer\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import json\n",
    "import pickle\n",
    "import nltk.data\n",
    "# from pyvi import ViTokenizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from gensim import corpora, matutils\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCJec1evOyb1"
   },
   "source": [
    "# Pre-process Post Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhpZth9kEHWi"
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "    \n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def vn_word_to_telex_type(word):\n",
    "    \"\"\"\n",
    "    Chuyển tiếng tiếng việt có dấu về kiểu gõ không bật unicode\n",
    "    \"\"\"\n",
    "    dau_cau = 0\n",
    "    new_word = ''\n",
    "    for char in word:\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            new_word += char\n",
    "            continue\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "        new_word += bang_nguyen_am[x][-1]\n",
    "    new_word += bang_ky_tu_dau[dau_cau]\n",
    "    return new_word\n",
    "\n",
    "\n",
    "def vn_sentence_to_telex_type(sentence):\n",
    "    \"\"\"\n",
    "    Chuyển câu tiếng việt có dấu về kiểu gõ telex.\n",
    "\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        words[index] = vn_word_to_telex_type(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "        else:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    sentence = sentence\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOpb5_oqEPgU"
   },
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    text = re.sub(url_regex, \" URL_TAG \", text)\n",
    "    return text\n",
    "  \n",
    "def remove_punctuation(text):\n",
    "    punc_regex = r'([!,?;.\\*\\\"\\[\\]\\&\\'\\#\\(\\^\\`\\~\\|\\{\\}\\)\\%])\\1+|([!,?;.\\*\\\"\\[\\]\\&\\'\\#\\(\\^\\`\\~\\|\\{\\}\\)\\%])'\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(punc_regex, '', corrected)\n",
    "    return corrected\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
    "    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
    "    return corrected.strip(\" \")\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  \n",
    "                               u\"\\U0001F300-\\U0001F5FF\" \n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                               u\"\\U00002500-\\U00002BEF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "EMAIL = re.compile(r\"([\\w0-9_\\.-:]+)(@)([\\d\\w\\.-]+)(\\.)([\\w\\.]{1,9})\")\n",
    "PHONE = re.compile(r\"(09|01[2|6|8|9]) + ([0-9]{8})\\b\")\n",
    "MENTION = re.compile(r\"@.+?:\")\n",
    "NUMBER =  re.compile(r\"\\d+.?\\d*\")\n",
    "DATETIME = '\\d{1,2}\\s?[/-]\\s?\\d{1,2}\\s?[/-]\\s?\\d{4}'\n",
    "\n",
    "RE_HTML_TAG = re.compile(r'<[^>]+>')\n",
    "RE_CLEAR_1 = re.compile(r'[^_<>\\s\\{Latin}]')\n",
    "RE_CLEAR_2 = re.compile(r'__+')\n",
    "RE_CLEAR_3 = re.compile(r'\\s+')\n",
    "\n",
    "\n",
    "def replace_common_token(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = re.sub(EMAIL, ' EMAIL_TAG ', text)\n",
    "    text = re.sub(PHONE, ' PHONE_TAG ', text)\n",
    "    text = re.sub(MENTION, ' ', text)\n",
    "    text = re.sub(NUMBER, ' NUMBER_TAG ', text)\n",
    "    text = re.sub(DATETIME, ' DATETIME_TAG ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_character(text):\n",
    "    text = re.sub(':v', '', text)\n",
    "    text = re.sub(':D', '', text)\n",
    "    text = re.sub(':3', '', text)\n",
    "    text = re.sub(':\\(', '', text)\n",
    "    text = re.sub(':\\)', '', text)\n",
    "    text = re.sub(':\\)\\)', '', text)\n",
    "    text = re.sub(':\\(\\(', '', text)\n",
    "    text = re.sub(':\\:', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_html_tag(text):\n",
    "    return re.sub(RE_HTML_TAG, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC3OcqBJEdtb"
   },
   "outputs": [],
   "source": [
    "def clear_sub(text, tokenize=True):\n",
    "    text = remove_html_tag(text)\n",
    "    text = remove_url(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = re.sub('&.{3,4};', ' ', str(text))\n",
    "    text = convert_unicode(text)\n",
    "    text = replace_common_token(text)\n",
    "    text = remove_character(text)\n",
    "    text = re.sub(RE_CLEAR_2, ' ', text)\n",
    "    text = re.sub(RE_CLEAR_3, ' ', text)\n",
    "    text = chuan_hoa_dau_cau_tieng_viet(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jla7cqdPEkCy"
   },
   "outputs": [],
   "source": [
    "rdrsegmenter = VnCoreNLP(\"/content/gdrive/My Drive/VLSP/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "def preprocess(text):\n",
    "    try :\n",
    "        text = clear_sub(text)  \n",
    "        sents = rdrsegmenter.tokenize(text)\n",
    "        text = ' '.join([' '.join(sent) for sent in sents])\n",
    "    except:\n",
    "        text = ''\n",
    "        print(\"xitj\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N39Nx9AoEyur"
   },
   "outputs": [],
   "source": [
    "path_train = \"path_to_dataset_train\"\n",
    "\n",
    "def build_dataset(path):\n",
    "    datas = []\n",
    "    labels = []\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    df['post_message'] = df['post_message'].apply(preprocess)\n",
    "    for data in df.values:\n",
    "        labels.append(data[7])\n",
    "    return list(df['post_message']), labels    \n",
    "\n",
    "train_text, train_label = build_dataset(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx-fMXoyRTUi"
   },
   "source": [
    "# **Build Classifer for meta data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pkH8QBeOycD"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDo9tfg3abHN"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_features = n_feature\n",
    "        self.fc1 = nn.Linear(self.n_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ct5ys5rOycE"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEBmzaz4RS0j"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"path_to_train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l99wkR6yRwxm"
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "train_data[\"post_message\"] = train_data[\"post_message\"].fillna(\"\")\n",
    "train_data = train_data.fillna(0)\n",
    "\n",
    "train_data = train_data[pd.to_numeric(train_data['timestamp_post'], errors='coerce').notnull()]\n",
    "\n",
    "def preprocess(x):\n",
    "    x = str(x)\n",
    "    try:\n",
    "        x = x.split()[0]\n",
    "        x = int(x)\n",
    "    except:\n",
    "        x = 0\n",
    "    return x\n",
    "\n",
    "train_data[\"timestamp_post\"] = train_data[\"timestamp_post\"].astype(float)\n",
    "train_data[\"num_like_post\"] = train_data[\"num_like_post\"].apply(lambda x: preprocess(x)) \n",
    "train_data[\"num_share_post\"] = train_data[\"num_share_post\"].apply(lambda x: preprocess(x)) \n",
    "train_data[\"num_comment_post\"] = train_data[\"num_comment_post\"].apply(lambda x: preprocess(x)) \n",
    "\n",
    "# Get images count in each post\n",
    "def get_num_image(id):\n",
    "    img_path = os.path.join(\"public_train_final_images\", str(id))\n",
    "    try:\n",
    "        num_image = len(os.listdir(img_path))\n",
    "    except:\n",
    "        num_image = 0\n",
    "    return num_image\n",
    "\n",
    "train_data['num_image'] = [get_num_image(id) for id in train_data['id']] \n",
    "\n",
    "# Get more feature from timestamp\n",
    "train_data['datetime'] = pd.to_datetime([datetime.fromtimestamp(timestamp)  for timestamp in train_data['timestamp_post']]) \n",
    "train_data[\"minute in hour\"] = [x.minute for x in train_data[\"datetime\"].dt.time]\n",
    "train_data[\"hour in day\"] = [x.hour for x in train_data[\"datetime\"].dt.time]\n",
    "train_data[\"day in month\"] = [x.day for x in train_data[\"datetime\"].dt.date]\n",
    "train_data[\"quarter in year\"] = [x.month % 4 for x in train_data[\"datetime\"].dt.date]\n",
    "train_data[\"month in year\"] = [x.month for x in train_data[\"datetime\"].dt.date]\n",
    "train_data[\"weekday\"] = [x.weekday() for x in train_data[\"datetime\"].dt.date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ_O1A8sSD2z"
   },
   "outputs": [],
   "source": [
    "meta_data =  train_data.drop([\"label\", \"id\", \"user_name\", \"post_message\", \"datetime\", \"timestamp_post\"], axis=1)\n",
    "X = meta_data.values\n",
    "y = train_data[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOeASRrfST4g"
   },
   "outputs": [],
   "source": [
    "pos, neg = np.bincount(train_df['label'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MNy5W6NOycH"
   },
   "source": [
    "# Build Multi-Input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyXPC3HISYss"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIKZxDM6SdQu"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiA5IFKdQKEP"
   },
   "source": [
    "### Load pretrained Meta Submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OT1l9HYoSnXO"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'path_to_model_trained_MLP'\n",
    "old_model = torch.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY7LQp35TgKQ"
   },
   "outputs": [],
   "source": [
    "class New(nn.Module):\n",
    "    \n",
    "    def __init__(self, MODEL_PATH):\n",
    "        super(New, self).__init__()\n",
    "        old_model = torch.load(MODEL_PATH)\n",
    "        modules = list(old_model.children())[:-1]\n",
    "        self.new_model = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.new_model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qc6mko_KQdp5"
   },
   "source": [
    "### Meta Submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3tOi4Nalu7w"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_features = n_feature\n",
    "        self.fc1 = nn.Linear(self.n_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "        self.fc4 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return torch.sigmoid(self.fc4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-r_PRa2TpUX"
   },
   "outputs": [],
   "source": [
    "new_model = New(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecxGz33Cl2bV"
   },
   "outputs": [],
   "source": [
    "new_model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFxXLza5enS9"
   },
   "outputs": [],
   "source": [
    "model_path = 'path_to_finetune_phobert_model_trained'\n",
    "class RoberTa(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        super(RoberTa, self).__init__()\n",
    "        old_model = torch.load(model_path)\n",
    "        modules = list(old_model.children())[:-1]\n",
    "        self.new_model = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.new_model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As0lP2GZF9y4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, BertPreTrainedModel\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import RobertaConfig, AdamW\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWOCXiP5GEda"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.15, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j74i8_WYPJah"
   },
   "source": [
    "### Load pretrained Text Submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQDA_FQRnX0O"
   },
   "outputs": [],
   "source": [
    "class LoadRobertaClassificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        # self.dense1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.Linear(config.hidden_size * 12, config.hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, -1, :]  # take <s> token (equiv. to [CLS])\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.dense1(x)\n",
    "        # x = torch.tanh(x)\n",
    "        x = self.dropout1(x) # 0.9178\n",
    "        x = self.dense2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class LoadRobertaTweetClassification(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = LoadRobertaClassificationHead(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        sequence_output = torch.cat(outputs[2][1:13], -1)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return sequence_output[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dtec8KpmGILI"
   },
   "outputs": [],
   "source": [
    "class RobertaClassificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        # self.dense1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.Linear(config.hidden_size * 12 + 256, config.hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        # x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.dense1(x)\n",
    "        # x = torch.tanh(x)\n",
    "        x = features\n",
    "        x = self.dropout1(x) # 0.9178\n",
    "        x = self.dense2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class RobertaTweetClassification(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = LoadRobertaTweetClassification.from_pretrained(\n",
    "                        \"path_to_finetune_phobert_model_trained\",\n",
    "                        config=config)\n",
    "        self.numberic_model = New(MODEL_PATH)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        self.loss_fct = CrossEntropyLoss() # LabelSmoothingLoss(classes=2, smoothing=0.15) # CrossEntropyLoss() # LabelSmoothingLoss(classes=2, smoothing=0.3)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        numberic_feature=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        sequence_output = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        numberic_encode = self.numberic_model(numberic_feature)\n",
    "        two_encode = torch.cat([sequence_output, numberic_encode], axis=-1)\n",
    "\n",
    "        logits = self.classifier(two_encode)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "          \n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBHGmb1HPxnJ"
   },
   "source": [
    "### Text Submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLo6pHZpzOR2"
   },
   "outputs": [],
   "source": [
    "class RobertaClassificationHead_0(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        # self.dense1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.Linear(config.hidden_size + 256, config.hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        # x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.dense1(x)\n",
    "        # x = torch.tanh(x)\n",
    "        x = features\n",
    "        x = self.dropout1(x) # 0.9178\n",
    "        x = self.dense2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class RobertaTweetClassification_0(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.numberic_model = New(MODEL_PATH)\n",
    "        self.classifier = RobertaClassificationHead_0(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        numberic_feature=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        sequence_output = torch.cat(outputs[2][1:13], -1)[:, 0, :]\n",
    "        numberic_encode = self.numberic_model(numberic_feature)\n",
    "        two_encode = torch.cat([sequence_output, numberic_encode], axis=-1)\n",
    "\n",
    "        logits = self.classifier(two_encode)\n",
    "\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss() # LabelSmoothingLoss(classes=2, smoothing=0.15) # CrossEntropyLoss() # LabelSmoothingLoss(classes=2, smoothing=0.3)\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82UZw8hPGaAJ"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import RobertaConfig, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoGpbfhsGmZB"
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    y_scores = preds[:, 1].flatten()\n",
    "    roc_auc = roc_auc_score(labels_flat, y_scores)\n",
    "    F1_score = f1_score(pred_flat, labels_flat)\n",
    "\n",
    "    return accuracy_score(pred_flat, labels_flat), F1_score, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP1YQvURe5B1"
   },
   "outputs": [],
   "source": [
    "train_label = np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp4NRLQtaX6j"
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda'\n",
    "seed_val = 2048\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "loss_values = []\n",
    "\n",
    "kf = KFold(n_splits = 10, random_state=42)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_text, X_train, train_label)):\n",
    "    if fold==0:\n",
    "        train_index = train_index\n",
    "        valid_index = valid_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jifmgk-OOycL"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDoZs07QV70T"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "# Roberta\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"vinai/phobert-base\", from_tf=False, num_labels = 2, output_hidden_states=True,\n",
    ")\n",
    "BERTweet = RobertaTweetClassification(config)\n",
    "\n",
    "# Creating optimizer and lr schedulers\n",
    "param_optimizer = list(BERTweet.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0005}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n",
    "BERTweet.to(device)\n",
    "frozen = True\n",
    "params = list(BERTweet.named_parameters())\n",
    "\n",
    "for p in params[:-2]:\n",
    "    p[1].requires_grad = False\n",
    "    \n",
    "scheduler0 = get_constant_schedule(optimizer)\n",
    "\n",
    "print(train_index)\n",
    "train_texts = np.array(train_text)[train_index]\n",
    "valid_texts = np.array(train_text)[valid_index]\n",
    "train_numberic = X_train[train_index]\n",
    "valid_numberic = X_train[valid_index]\n",
    "train_labels = np.array(train_label)[train_index]\n",
    "valid_labels = np.array(train_label)[valid_index]\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n",
    "\n",
    "train_ids = torch.tensor(train_encodings['input_ids'])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_numberic = torch.tensor(train_numberic)\n",
    "valid_ids = torch.tensor(valid_encodings['input_ids'])\n",
    "valid_labels = torch.tensor(valid_labels)\n",
    "valid_masks = torch.tensor(valid_encodings['attention_mask'])\n",
    "valid_numberic = torch.tensor(valid_numberic)\n",
    "train_data = TensorDataset(train_ids, train_masks, train_numberic, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "valid_data = TensorDataset(valid_ids, valid_masks, valid_numberic, valid_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=32)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                    num_warmup_steps = 30,\n",
    "                                    num_training_steps = total_steps)\n",
    "roc_max = 0\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    BERTweet.train()\n",
    "    train_accuracy = 0\n",
    "    nb_train_steps = 0\n",
    "    train_f1 = 0\n",
    "\n",
    "    if epoch_i > 0 and frozen:\n",
    "        params = list(BERTweet.named_parameters())\n",
    "        for p in params[:-2]:\n",
    "            p[1].requires_grad = True\n",
    "        frozen = False\n",
    "        del scheduler0\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    for  batch in tqdm(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_numberic = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        BERTweet.zero_grad()\n",
    "        \n",
    "        outputs = BERTweet(b_input_ids, \n",
    "            token_type_ids=None,\n",
    "            numberic_feature = b_numberic, \n",
    "            attention_mask=b_input_mask, \n",
    "            labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(BERTweet.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not frozen:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler0.step()\n",
    "\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\" Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    BERTweet.eval()\n",
    "    eval_loss = 0\n",
    "    \n",
    "    total_labels = []\n",
    "    total_pre = []\n",
    "    \n",
    "    for batch in tqdm(valid_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_numberic, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = BERTweet(b_input_ids, \n",
    "            token_type_ids=None,\n",
    "            numberic_feature=b_numberic,\n",
    "            attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            logits = softmax(logits, axis=1)\n",
    "            total_pre.extend(logits[:,1].flatten())\n",
    "            total_labels.extend(label_ids.flatten())\n",
    "\n",
    "    roc_auc = roc_auc_score(total_labels, total_pre)\n",
    "\n",
    "    if roc_auc > roc_max:\n",
    "        roc_max = roc_auc\n",
    "        print(\"*\"*100)\n",
    "        torch.save(BERTweet.state_dict(), \"model_{}.bin\".format(epoch_i))\n",
    "    \n",
    "    print(roc_auc)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISiCW7qfOycM"
   },
   "source": [
    "# Evaluate in TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSd3wuHZGtH9"
   },
   "outputs": [],
   "source": [
    "# Roberta\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"vinai/phobert-base\", from_tf=False, num_labels = 2, output_hidden_states=True)\n",
    "\n",
    "BERTweet = RobertaTweetClassification.from_pretrained(\n",
    "    \"path_to_model_trained\",\n",
    "    config=config)\n",
    "\n",
    "BERTweet.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdnM81FJRObl"
   },
   "outputs": [],
   "source": [
    "path_test = \"path_to_dataset_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xStenGyrRpmh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def build_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['post_message'] = df['post_message'].apply(preprocess)\n",
    "    return list(df['post_message'])   \n",
    "\n",
    "test_text = build_dataset(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVC5qA38R5NK"
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_text, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI8GRwXETLaV"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "test_ids = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "test_data = TensorDataset(test_ids, test_masks)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QOu4T38TPw9"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "print(\"Running Test...\")\n",
    "BERTweet.eval()\n",
    "eval_loss = 0\n",
    "\n",
    "total_pre = []\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # print(b_input_ids)\n",
    "    with torch.no_grad():\n",
    "        outputs = BERTweet(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        logits = softmax(logits, axis=1)\n",
    "        total_pre.extend(logits[:,1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgSTq4yixlbz"
   },
   "outputs": [],
   "source": [
    "print(total_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TbyOvcKTUIq"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path_test)\n",
    "with open('results.csv', 'w') as f_w:\n",
    "    for id, logit in zip(list(test_df['id']), total_pre):\n",
    "        f_w.write(str(id) + ',' + str(logit) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RVL7nQ3TgAr"
   },
   "outputs": [],
   "source": [
    "var = {}\n",
    "for i in range(9):\n",
    "    datas= []\n",
    "    with open ('results_{}.csv'.format(i), 'r') as f_w:\n",
    "        for data in f_w:\n",
    "            data = data.strip().split(\",\")[1]\n",
    "            datas.append(float(data))\n",
    "            var[str(i)] = datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYTrZAGix10-"
   },
   "outputs": [],
   "source": [
    "print(len(var['0']))\n",
    "average_pred = []\n",
    "for i in range(len(var['0'])):\n",
    "    data = (var['0'][i] + var['1'][i] + var['2'][i] + var['3'][i] + var['4'][i] + var['5'][i] + var['6'][i] + var['7'][i] + var['8'][i])/9\n",
    "    average_pred.append(str(data))\n",
    "print(average_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T79y18mu2uPQ"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path_test)\n",
    "with open('results.csv', 'w') as f_w:\n",
    "    for id, logit in zip(list(test_df['id']), average_pred):\n",
    "        f_w.write(str(id) + ',' + str(logit) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odMZEY0rNdR3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Multi-input model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
