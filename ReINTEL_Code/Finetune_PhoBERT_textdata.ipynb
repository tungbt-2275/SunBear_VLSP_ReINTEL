{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import nessesary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1607607041876,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "0pX8dP29DlJ6"
   },
   "outputs": [],
   "source": [
    "!pip install vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1607607065755,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "8jKQfcEpDrRN"
   },
   "outputs": [],
   "source": [
    "!pip install vncorenlp transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1KyKKH8Dyuz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "from csv import reader\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import traceback\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import json\n",
    "import pickle\n",
    "import nltk.data\n",
    "from sklearn.svm import LinearSVC\n",
    "from gensim import corpora, matutils\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Post Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhpZth9kEHWi"
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def vn_word_to_telex_type(word):\n",
    "    dau_cau = 0\n",
    "    new_word = ''\n",
    "    for char in word:\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            new_word += char\n",
    "            continue\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "        new_word += bang_nguyen_am[x][-1]\n",
    "    new_word += bang_ky_tu_dau[dau_cau]\n",
    "    return new_word\n",
    "\n",
    "\n",
    "def vn_sentence_to_telex_type(sentence):\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        words[index] = vn_word_to_telex_type(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "        else:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "\n",
    "        \"\"\"\n",
    "    sentence = sentence\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOpb5_oqEPgU"
   },
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url_regex = r'<URL>'\n",
    "    text = re.sub(url_regex, \"URL_TAG\", text)\n",
    "    return text\n",
    "  \n",
    "def remove_punctuation(text):\n",
    "    punc_regex = r'([!,?;.\\*\\\"\\[\\]\\&\\'\\#\\(\\^\\`\\~\\|\\{\\}\\)\\%])\\1+|([!,?;.\\*\\\"\\[\\]\\&\\'\\#\\(\\^\\`\\~\\|\\{\\}\\)\\%])\\1+'\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(punc_regex, r'\\1', corrected)\n",
    "    return corrected\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
    "    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
    "    return corrected.strip(\" \")\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                               u\"\\U0001F680-\\U0001F6FF\" \n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                               u\"\\U00002500-\\U00002BEF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\" \n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "EMAIL = re.compile(r\"([\\w0-9_\\.-:]+)(@)([\\d\\w\\.-]+)(\\.)([\\w\\.]{1,9})\")\n",
    "PHONE = re.compile(r\"(09|01[2|6|8|9]) + ([0-9]{8})\\b\")\n",
    "MENTION = re.compile(r\"@.+?:\")\n",
    "NUMBER =  re.compile(r\"\\d+.?\\d*\")\n",
    "DATETIME = '\\d{1,2}\\s?[/-]\\s?\\d{1,2}\\s?[/-]\\s?\\d{4}'\n",
    "\n",
    "RE_HTML_TAG = re.compile(r'<[^>]+>')\n",
    "RE_CLEAR_1 = re.compile(r'[^_<>\\s\\{Latin}]')\n",
    "RE_CLEAR_2 = re.compile(r'__+')\n",
    "RE_CLEAR_3 = re.compile(r'\\s+')\n",
    "\n",
    "\n",
    "def replace_common_token(text):\n",
    "    text = remove_whitespace(text)\n",
    "    text = re.sub(EMAIL, ' EMAIL_TAG ', text)\n",
    "    text = re.sub(PHONE, ' PHONE_TAG ', text)\n",
    "    text = re.sub(MENTION, ' MENTION_TAG ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_character(text):\n",
    "    text = re.sub(':v', '', text)\n",
    "    text = re.sub(':D', '', text)\n",
    "    text = re.sub(':3', '', text)\n",
    "    text = re.sub(':\\(', '', text)\n",
    "    text = re.sub(':\\)', '', text)\n",
    "    text = re.sub(':\\)\\)', '', text)\n",
    "    text = re.sub(':\\(\\(', '', text)\n",
    "    text = re.sub(':\\:', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_html_tag(text):\n",
    "    return re.sub(RE_HTML_TAG, ' HTML_TAG ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC3OcqBJEdtb"
   },
   "outputs": [],
   "source": [
    "def clear_sub(text, tokenize=True):\n",
    "    text = remove_url(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = re.sub('&.{3,4};', ' ', str(text))\n",
    "    text = convert_unicode(text)\n",
    "    text = replace_common_token(text)\n",
    "    text = remove_character(text)\n",
    "    text = chuan_hoa_dau_cau_tieng_viet(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7310,
     "status": "ok",
     "timestamp": 1607187671822,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "jla7cqdPEkCy",
    "outputId": "dac2d445-038a-420c-f580-00212d905e08"
   },
   "outputs": [],
   "source": [
    "rdrsegmenter = VnCoreNLP(\"/content/gdrive/My Drive/VLSP/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "def preprocess(text):\n",
    "    try :\n",
    "        text = clear_sub(text)  \n",
    "        sents = rdrsegmenter.tokenize(text)\n",
    "        text = ' '.join([' '.join(sent) for sent in sents])\n",
    "    except:\n",
    "        text = ''\n",
    "        print(\"xitj\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1607606952491,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "N39Nx9AoEyur"
   },
   "outputs": [],
   "source": [
    "path_train = \"path_to_dataset_train\"\n",
    "def build_dataset(path):\n",
    "    datas = []\n",
    "    labels = []\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    df['post_message'] = df['post_message'].apply(preprocess)\n",
    "    for data in df.values:\n",
    "        try:\n",
    "            labels.append(int(data[7]))\n",
    "        except:\n",
    "            print(data) \n",
    "    return list(df['post_message']), labels    \n",
    "\n",
    "train_text, train_label = build_dataset(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1607606393031,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "ZbN8yNo3FEMn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As0lP2GZF9y4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, BertPreTrainedModel\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import RobertaConfig, AdamW\n",
    "from torch.nn import CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWOCXiP5GEda"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.15, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1607606427393,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "Dtec8KpmGILI"
   },
   "outputs": [],
   "source": [
    "class RobertaClassificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        # self.dense1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.Linear(config.hidden_size * 12, config.hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, -1, :]  # take <s> token (equiv. to [CLS])\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.dense1(x)\n",
    "        # x = torch.tanh(x)\n",
    "        x = self.dropout1(x) # 0.9178\n",
    "        x = self.dense2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class RobertaTweetClassification(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        sequence_output = torch.cat(outputs[2][1:13], -1)\n",
    "        \n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = LabelSmoothingLoss(classes=2, smoothing=0.12) # CrossEntropyLoss() # LabelSmoothingLoss(classes=2, smoothing=0.3)\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoGpbfhsGmZB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    y_scores = preds[:, 1].flatten()\n",
    "    roc_auc = roc_auc_score(labels_flat, y_scores)\n",
    "    F1_score = f1_score(pred_flat, labels_flat)\n",
    "    return accuracy_score(pred_flat, labels_flat), F1_score, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1607606489165,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "Kp4NRLQtaX6j"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "import random\n",
    "import time\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import RobertaConfig, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda'\n",
    "loss_values = []\n",
    "\n",
    "kf = KFold(n_splits = 8)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_text, train_label)):\n",
    "\n",
    "    epochs = 15\n",
    "\n",
    "    # Roberta\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "      \"vinai/phobert-base\", from_tf=False, num_labels = 2, output_hidden_states=True,\n",
    "    )\n",
    "    BERTweet = RobertaTweetClassification.from_pretrained(\n",
    "      \"vinai/phobert-base\",\n",
    "      config=config\n",
    "    )\n",
    "\n",
    "    param_optimizer = list(BERTweet.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0005}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n",
    "    BERTweet.to(device)\n",
    "    frozen = True\n",
    "    params = list(BERTweet.named_parameters())\n",
    "    for p in params[:-2]:\n",
    "        p[1].requires_grad = False \n",
    "  \n",
    "    scheduler0 = get_constant_schedule(optimizer)\n",
    "\n",
    "    print(train_index)\n",
    "    train_texts = np.array(train_text)[train_index]\n",
    "    valid_texts = np.array(train_text)[valid_index]\n",
    "    train_labels = np.array(train_label)[train_index]\n",
    "    valid_labels = np.array(train_label)[valid_index]\n",
    "    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "    valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n",
    "\n",
    "    train_ids = torch.tensor(train_encodings['input_ids'])\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "\n",
    "    valid_ids = torch.tensor(valid_encodings['input_ids'])\n",
    "    valid_labels = torch.tensor(valid_labels)\n",
    "    valid_masks = torch.tensor(valid_encodings['attention_mask'])\n",
    "\n",
    "    train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "    valid_data = TensorDataset(valid_ids, valid_masks, valid_labels)\n",
    "    valid_sampler = SequentialSampler(valid_data)\n",
    "    valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=32)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                      num_warmup_steps = 20,\n",
    "                                      num_training_steps = total_steps)\n",
    "    roc_max = 0\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_loss = 0\n",
    "        BERTweet.train()\n",
    "        train_accuracy = 0\n",
    "        nb_train_steps = 0\n",
    "        train_f1 = 0\n",
    "\n",
    "        if epoch_i > 0 and frozen:\n",
    "            params = list(BERTweet.named_parameters())\n",
    "            for p in params[:-2]:\n",
    "              p[1].requires_grad = True\n",
    "\n",
    "            frozen = False\n",
    "            del scheduler0\n",
    "            torch.cuda.empty_cache()\n",
    "      \n",
    "        for  batch in tqdm(train_dataloader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            BERTweet.zero_grad()\n",
    "            outputs = BERTweet(b_input_ids, \n",
    "              token_type_ids=None, \n",
    "              attention_mask=b_input_mask, \n",
    "              labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(BERTweet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "          \n",
    "            if not frozen:\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                scheduler0.step()\n",
    "\n",
    "          \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        loss_values.append(avg_train_loss)\n",
    "        print(\" Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "\n",
    "\n",
    "        print(\"Running Validation...\")\n",
    "        t0 = time.time()\n",
    "        BERTweet.eval()\n",
    "        eval_loss = 0\n",
    "\n",
    "        total_labels = []\n",
    "        total_pre = []\n",
    "      \n",
    "        for batch in tqdm(valid_dataloader):\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            b_input_ids, b_input_mask,\n",
    "            b_labels = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = BERTweet(b_input_ids, \n",
    "                token_type_ids=None, \n",
    "                attention_mask=b_input_mask)\n",
    "                logits = outputs[0]\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                logits = softmax(logits, axis=1)\n",
    "                total_pre.extend(logits[:,1].flatten())\n",
    "                total_labels.extend(label_ids.flatten())\n",
    "\n",
    "        roc_auc = roc_auc_score(total_labels, total_pre)\n",
    "\n",
    "        if roc_auc > roc_max:\n",
    "            roc_max = roc_auc\n",
    "            print(\"*\"*100)\n",
    "            torch.save(BERTweet.state_dict(), \"path_to_folder_save_model_{}\".format(fold))\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdnM81FJRObl"
   },
   "outputs": [],
   "source": [
    "path_test = \"path_to_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xStenGyrRpmh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def build_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['post_message'] = df['post_message'].apply(preprocess)\n",
    "    return list(df['post_message'])   \n",
    "\n",
    "test_text = build_dataset(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVC5qA38R5NK"
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_text, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI8GRwXETLaV"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "test_ids = torch.tensor(test_encodings['input_ids'])\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "test_data = TensorDataset(test_ids, test_masks)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1607606740208,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "LdakEb-nV3dD"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "device = 'cuda'\n",
    "test_df = pd.read_csv(path_test)\n",
    "for i in range(6,7):\n",
    "    print(i)\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "    \"vinai/phobert-base\", from_tf=False, num_labels = 2, output_hidden_states=True,\n",
    "    )\n",
    "    BERTweet = RobertaTweetClassification.from_pretrained(\n",
    "      \"path-to_folder_saved_model\".format(i),\n",
    "      config=config\n",
    "\n",
    "    )\n",
    "\n",
    "    BERTweet.to('cuda')\n",
    "    print(\"Running Test...\")\n",
    "    BERTweet.eval()\n",
    "    eval_loss = 0\n",
    "\n",
    "    total_pre = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = BERTweet(b_input_ids, \n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            logits = softmax(logits, axis=1)\n",
    "            total_pre.extend(logits[:,1].flatten())\n",
    "\n",
    "    with open('path_to_test'.format(i), 'w') as f_w:\n",
    "        for id, logit in zip(list(test_df['id']), total_pre):\n",
    "            f_w.write(str(id) + ',' + str(logit) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RVL7nQ3TgAr"
   },
   "outputs": [],
   "source": [
    "var = {}\n",
    "for i in range(6,7):\n",
    "    datas= []\n",
    "    with open ('/content/gdrive/MyDrive/file/results_{}.csv'.format(i), 'r') as f_w:\n",
    "        for data in f_w:\n",
    "            data = data.strip().split(\",\")[1]\n",
    "            datas.append(float(data))\n",
    "            var[str(i)] = datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1607606593189,
     "user": {
      "displayName": "Hoang Trinh Viet",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjxVraZMjiRtGQJ_VUvH-YPtaoUh90kKBJDxNTn=s64",
      "userId": "12507488114828875639"
     },
     "user_tz": -420
    },
    "id": "lYTrZAGix10-"
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(len(var['9'])):\n",
    "    data =var['6'][i]\n",
    "    data = max(var['0'][i], var['1'][i], var['2'][i], var['3'][i], var['4'][i], var['5'][i], var['6'][i], var['7'][i], var['8'][i])\n",
    "    pred.append(str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T79y18mu2uPQ"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path_test)\n",
    "with open('/content/gdrive/MyDrive/file/results.csv', 'w') as f_w:\n",
    "    for id, logit in zip(list(test_df['id']), pred):\n",
    "        f_w.write(str(id) + ',' + str(logit) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXOQdTx6m4mw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Finetune_PhoBERT_textdata.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
